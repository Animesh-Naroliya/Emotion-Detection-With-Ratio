# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cZubh5dwLUt4YOBq49IazJ7VXcKw-0xO
"""

from keras.models import load_model
from time import sleep
from keras.preprocessing.image import img_to_array
import cv2
import numpy as np

# Load the pre-trained model and cascade classifier
classifier = load_model(r'/Users/animeshnaroliyagmail.com/Evening Classes/model.h5')
face_classifier = cv2.CascadeClassifier(r'/Users/animeshnaroliyagmail.com/Evening Classes/haarcascade_frontalface_default.xml')

# Define emotion labels
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

# Initialize video capture
cap = cv2.VideoCapture(0)

# Variables to store counts for each emotion class
emotion_counts = {label: 0 for label in emotion_labels}

while True:
    # Read a frame from the video capture
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if frame is not captured successfully

    # Convert frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the grayscale frame
    faces = face_classifier.detectMultiScale(gray)

    # Calculate emotion ratios based on the total number of faces detected
    total_faces = len(faces)
    if total_faces > 0:
        # Filter out small faces
        filtered_faces = [(x, y, w, h) for (x, y, w, h) in faces if w * h > 5000]

        # Recalculate total faces based on filtered faces
        total_faces = len(filtered_faces)

        if total_faces > 0:
            emotion_counts = {label: 0 for label in emotion_labels}  # Reset emotion counts

            for (x, y, w, h) in filtered_faces:
                # Extract the region of interest (ROI) containing the face
                roi_gray = gray[y:y+h, x:x+w]
                roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)

                # Check if the ROI contains a face
                if np.sum([roi_gray]) != 0:
                    # Preprocess the ROI for prediction
                    roi = roi_gray.astype('float') / 255.0
                    roi = img_to_array(roi)
                    roi = np.expand_dims(roi, axis=0)

                    # Predict emotion using the pre-trained model
                    prediction = classifier.predict(roi)[0]
                    label = emotion_labels[prediction.argmax()]
                    emotion_counts[label] += 1

            # Normalize emotion counts to calculate ratios
            total_emotions = sum(emotion_counts.values())
            ratios = {label: round(count / total_emotions * 100, 2) for label, count in emotion_counts.items()}
        else:
            ratios = {label: 0 for label in emotion_labels}
    else:
        ratios = {label: 0 for label in emotion_labels}

    # Display emotion ratios
    for label, ratio in ratios.items():
        text = f"{label}: {ratio:.2f}%"
        cv2.putText(frame, text, (20, 40 + emotion_labels.index(label) * 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)  # Changed color parameter to black

    # Display the frame with emotion labels
    cv2.imshow('Emotion Detector', frame)

    # Break the loop if 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture object and close OpenCV windows
cap.release()
cv2.destroyAllWindows()